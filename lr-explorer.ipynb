{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring sophisticated learning rate strategies for deep learning with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the hardware backend (GPU/CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_cpu = False\n",
    "if torch.cuda.is_available() and not force_cpu:\n",
    "    use_gpu = True\n",
    "    FloatTensor = torch.cuda.FloatTensor\n",
    "    LongTensor = torch.cuda.LongTensor\n",
    "else:\n",
    "    use_gpu = False\n",
    "    FloatTensor = torch.FloatTensor\n",
    "    LongTensor = torch.LongTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions and classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mv_avg(l, n):\n",
    "    n = min(n, len(l))\n",
    "    s = sum(l[-n:])\n",
    "    return s / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes for keeping track of performance information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceHistory:\n",
    "    \n",
    "    def __init__(self, mv_avg_w=1):\n",
    "        self.losses = []\n",
    "        self.accs = []\n",
    "        self.mv_avg_w = mv_avg_w\n",
    "        self.mv_avg_losses = []\n",
    "        self.mv_avg_accs = []\n",
    "        \n",
    "    def update_history(self, loss, acc):\n",
    "        self.losses.append(loss)\n",
    "        mv_avg_loss = mv_avg(self.losses, self.mv_avg_w)\n",
    "        self.mv_avg_losses.append(mv_avg_loss)\n",
    "        self.accs.append(acc)\n",
    "        mv_avg_acc = mv_avg(self.accs, self.mv_avg_w)\n",
    "        self.mv_avg_accs.append(mv_avg_acc)\n",
    "        \n",
    "class TrainingHistory(PerformanceHistory):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(32)\n",
    "        self.lrs = []\n",
    "        \n",
    "    def update_history(self, lr, loss, acc):\n",
    "        super().update_history(loss, acc)\n",
    "        self.lrs.append(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "training_set_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(32, scale=(0.9, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "training_set = datasets.CIFAR10(root='CIFAR10_data', train=True,\n",
    "                               transform=training_set_transform,\n",
    "                               download=True)\n",
    "training_set_loader = torch.utils.data.DataLoader(training_set,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=True,\n",
    "                                                 num_workers=4)\n",
    "\n",
    "test_set_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "test_set = datasets.CIFAR10(root='CIFAR10_data', train=False,\n",
    "                           transform=test_set_transform,\n",
    "                           download=True)\n",
    "test_set_loader = torch.utils.data.DataLoader(test_set,\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=False,\n",
    "                                             num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, _ = next(iter(training_set_loader))\n",
    "grid = torchvision.utils.make_grid(images, normalize=True)\n",
    "grid = np.transpose(grid.numpy(), (1, 2, 0))\n",
    "plt.imshow(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBnLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels,\n",
    "                 kernel_size, stride, padding):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size,\n",
    "                              stride, padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.relu(self.bn(self.conv(x)))\n",
    "\n",
    "class ResLayer(ConvBnLayer):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + super().forward(x)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvBnLayer(3, 32, 5, 1, 2)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            ConvBnLayer(32, 64, 3, 2, 1),\n",
    "            ResLayer(64, 64, 3, 1, 1),\n",
    "            ResLayer(64, 64, 3, 1, 1))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            ConvBnLayer(64, 128, 3, 2, 1),\n",
    "            ResLayer(128, 128, 3, 1, 1),\n",
    "            ResLayer(128, 128, 3, 1, 1)\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(4)\n",
    "        self.fc = nn.Linear(2048, 10)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.dropout(x.view(x.size(0), -1))\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "def create_model():\n",
    "    model = CNN()\n",
    "    if use_gpu:\n",
    "        model = model.cuda()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the learning rate schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Scheduler:\n",
    "    \n",
    "    def __init__(self, optimizer):\n",
    "        self.i = 0\n",
    "        self.optimizer = optimizer\n",
    "        self.lr = 0\n",
    "        self.snapshot = False\n",
    "        \n",
    "    def calc_lr(self):\n",
    "        return 0\n",
    "    \n",
    "    def step(self):\n",
    "        self.lr = self.calc_lr()\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = self.lr\n",
    "        self.i += 1\n",
    "        return self.snapshot\n",
    "\n",
    "class CosineAnnealing(_Scheduler):\n",
    "    \n",
    "    def __init__(self, optimizer, min_lr, max_lr, cycle_len, cycle_mult):\n",
    "        super().__init__(optimizer)\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.cycle_len = cycle_len\n",
    "        self.i_max = self.cycle_len - 1\n",
    "        self.cycle_mult = cycle_mult\n",
    "        \n",
    "    def calc_lr(self):\n",
    "        # linearly scale iteration to be between 0 and pi\n",
    "        # so cosine is between -1 and 1\n",
    "        x = self.i / self.i_max * np.pi\n",
    "        # take cosine of scaled iteration and linearly\n",
    "        # scale it to be between min_lr and max_lr\n",
    "        lr = (self.max_lr - self.min_lr) / 2 * (np.cos(x) + 1) + self.min_lr\n",
    "        return lr\n",
    "    \n",
    "    def step(self):\n",
    "        _ = super().step()\n",
    "        if self.i > self.i_max:\n",
    "            self.i = 0\n",
    "            self.cycle_len *= self.cycle_mult\n",
    "            self.i_max = self.cycle_len - 1\n",
    "            self.snapshot = True\n",
    "        else:\n",
    "            self.snapshot = False\n",
    "        return self.snapshot\n",
    "\n",
    "class Exponential(_Scheduler):\n",
    "    \n",
    "    def __init__(self, optimizer, base_lr=5e-6, n=1.01):\n",
    "        super().__init__(optimizer)\n",
    "        self.base_lr = base_lr\n",
    "        self.n = n\n",
    "        \n",
    "    def calc_lr(self):\n",
    "        lr = self.base_lr * self.n ** self.i\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifier:\n",
    "    \n",
    "    def __init__(self, snapshot_ensemble_size=0):\n",
    "        self.model = create_model()\n",
    "        self.snapshots = []\n",
    "        self.snapshot_ensemble_size = snapshot_ensemble_size\n",
    "        self.optimizer = optim.Adam(self.model.parameters())\n",
    "        self.scheduler = None\n",
    "        self.train_history = TrainingHistory()\n",
    "        self.test_history = PerformanceHistory()\n",
    "\n",
    "    def forward_pass(self, model, data):\n",
    "        inputs, targets = data\n",
    "        inputs = Variable(inputs.type(FloatTensor))\n",
    "        targets = Variable(targets.type(LongTensor))\n",
    "        outputs = model(inputs)\n",
    "        _, predictions = outputs.max(1)\n",
    "        loss = F.cross_entropy(outputs, targets)\n",
    "        acc = (predictions == targets).sum().data[0] / batch_size\n",
    "        return loss, acc\n",
    "    \n",
    "    def lr_find(self, epochs=1):\n",
    "        lrf_model = copy.deepcopy(self.model)\n",
    "        lrf_optimizer = optim.Adam(lrf_model.parameters())\n",
    "        lrf_scheduler = Exponential(lrf_optimizer)\n",
    "        self.lrf_history = TrainingHistory()\n",
    "        for epoch in range(1, epochs+1):\n",
    "            with tqdm(training_set_loader,\n",
    "                      desc=\"[lr_find] Epoch %d/%d\" % (epoch, epochs),\n",
    "                      unit=\"batches\") as t:\n",
    "                for data in t:\n",
    "                    lrf_optimizer.zero_grad()\n",
    "                    loss, acc = self.forward_pass(lrf_model, data)\n",
    "                    loss.backward()\n",
    "                    _ = lrf_scheduler.step()\n",
    "                    lrf_optimizer.step()\n",
    "                    self.lrf_history.update_history(lrf_scheduler.lr,\n",
    "                                                    loss.data[0], acc)\n",
    "                    t.set_postfix(loss=self.lrf_history.mv_avg_losses[-1],\n",
    "                                  acc=self.lrf_history.mv_avg_accs[-1],\n",
    "                                  lr=lrf_scheduler.lr)\n",
    "                    starting_loss_i = min(31, len(self.lrf_history.mv_avg_losses)-1)\n",
    "                    loss_threshold = self.lrf_history.mv_avg_losses[starting_loss_i] * 1.3\n",
    "                    if self.lrf_history.mv_avg_losses[-1] > loss_threshold:\n",
    "                        break\n",
    "        print(\"Done!\")\n",
    "                        \n",
    "    def set_scheduler(self, scheduler, opts):\n",
    "        self.scheduler = scheduler(self.optimizer, *opts)\n",
    "\n",
    "    def train(self, epochs=10):\n",
    "        if self.scheduler == None:\n",
    "            print(\"Scheduler not yet set\")\n",
    "            return\n",
    "        for epoch in range(1, epochs+1):\n",
    "            with tqdm(training_set_loader,\n",
    "                      desc=\"[train] Epoch %d/%d\" % (epoch, epochs),\n",
    "                      unit=\"batches\") as t:\n",
    "                for data in t:\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss, acc = self.forward_pass(self.model, data)\n",
    "                    loss.backward()\n",
    "                    snapshot = self.scheduler.step()\n",
    "                    if self.snapshot_ensemble_size and snapshot:\n",
    "                        self.snapshots.append(copy.deepcopy(self.model))\n",
    "                        if len(self.snapshots) > self.snapshot_ensemble_size:\n",
    "                            del self.snapshots[0]\n",
    "                    self.optimizer.step()\n",
    "                    self.train_history.update_history(self.scheduler.lr,\n",
    "                                                      loss.data[0], acc)\n",
    "                    t.set_postfix(loss=self.train_history.mv_avg_losses[-1],\n",
    "                                  acc=self.train_history.mv_avg_accs[-1])\n",
    "            running_test_loss = 0.0\n",
    "            running_test_acc = 0.0\n",
    "            i = 1\n",
    "            if len(self.snapshots) == 0:\n",
    "                test_ensemble = [self.model]\n",
    "            else:\n",
    "                test_ensemble = self.snapshots\n",
    "            with tqdm(test_set_loader,\n",
    "                      desc=\"[test] Epoch %d/%d\" % (epoch, epochs),\n",
    "                      unit=\"batches\") as t:\n",
    "                for data in t:\n",
    "                    inputs, targets = data\n",
    "                    inputs = Variable(inputs.type(FloatTensor))\n",
    "                    targets = Variable(targets.type(LongTensor))\n",
    "                    mean_outputs = 0\n",
    "                    for model in test_ensemble:\n",
    "                        outputs = model(inputs)\n",
    "                        mean_outputs += outputs\n",
    "                    mean_outputs = mean_outputs / len(test_ensemble)\n",
    "                    _, predictions = mean_outputs.max(1)\n",
    "                    loss = F.cross_entropy(mean_outputs, targets)\n",
    "                    acc = (predictions == targets).sum().data[0] / batch_size\n",
    "                    running_test_loss += loss.data[0]\n",
    "                    running_test_acc += acc\n",
    "                    t.set_postfix(loss=running_test_loss/i,\n",
    "                                  acc=running_test_acc/i)\n",
    "                    i += 1\n",
    "            self.test_history.update_history(running_test_loss/len(test_set_loader),\n",
    "                                             running_test_acc/len(test_set_loader))\n",
    "        print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding optimal learning rate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = ImageClassifier()\n",
    "classifier.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(classifier.lrf_history.lrs,\n",
    "             classifier.lrf_history.mv_avg_losses)\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(classifier.lrf_history.lrs,\n",
    "             classifier.lrf_history.mv_avg_accs)\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with a static learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.set_scheduler(Exponential, (3e-4, 1))\n",
    "classifier.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3)\n",
    "axes[0].plot(classifier.train_history.lrs, color=\"Blue\")\n",
    "axes[0].set_ylabel(\"Learning Rate\", color=\"Blue\")\n",
    "axes[1].plot(classifier.train_history.mv_avg_losses, color=\"Red\")\n",
    "axes[1].set_ylabel(\"Loss\", color=\"Red\")\n",
    "axes[2].plot(classifier.train_history.mv_avg_accs, color=\"Green\")\n",
    "axes[2].set_ylabel(\"Accuracy\", color=\"Green\")\n",
    "axes[2].set_xlabel(\"Mini-batch\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with a cosine annealing learning rate scheduler and snapshot ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier2 = ImageClassifier(4)\n",
    "classifier2.set_scheduler(CosineAnnealing, (1e-5, 5e-4, len(training_set_loader)*2, 1))\n",
    "classifier2.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3)\n",
    "axes[0].plot(classifier2.train_history.lrs, color=\"Blue\")\n",
    "axes[0].set_ylabel(\"Learning Rate\", color=\"Blue\")\n",
    "axes[1].plot(classifier2.train_history.mv_avg_losses, color=\"Red\")\n",
    "axes[1].set_ylabel(\"Loss\", color=\"Red\")\n",
    "axes[2].plot(classifier2.train_history.mv_avg_accs, color=\"Green\")\n",
    "axes[2].set_ylabel(\"Accuracy\", color=\"Green\")\n",
    "axes[2].set_xlabel(\"Mini-batch\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2)\n",
    "axes[0].plot(np.arange(1, len(classifier.test_history.accs)+1),\n",
    "             classifier.test_history.losses, \n",
    "             color=\"Crimson\", label=\"Static LR\")\n",
    "axes[0].plot(np.arange(1, len(classifier2.test_history.accs)+1),\n",
    "             classifier2.test_history.losses, \n",
    "             color=\"Goldenrod\", label=\"CA w/ SE\")\n",
    "axes[0].legend()\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[1].plot(np.arange(1, len(classifier2.test_history.accs)+1),\n",
    "             classifier2.test_history.accs,\n",
    "             color=\"Darkgreen\", label=\"Static LR\")\n",
    "axes[1].plot(np.arange(1, len(classifier2.test_history.accs)+1),\n",
    "             classifier2.test_history.accs,\n",
    "             color=\"Blue\", label=\"CA w/ SE\")\n",
    "axes[1].set_ylabel(\"Accuracy\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
