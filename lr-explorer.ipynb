{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring sophisticated learning rate strategies for training deep neural networks\n",
    "In this notebook, we will explore some effective strategies for working with learning rate in deep neural networks, using the classification of CIFAR10 images as an example problem. These techniques include:\n",
    "\n",
    "* A systematic method for estimating optimal learning rate settings, in which the learning rate is constantly increased for a short duration of training, and then plotted against training loss or accuracy. This idea was introduced in [this paper](https://arxiv.org/abs/1506.01186) by Leslie N. Smith.\n",
    "\n",
    "* Time-based learning rate scheduling, and in particular cosine annealing with warm restarts, where the learning rate is cyclically varied between high and low boundaries in a cosine pattern. This particular technique comes from [this paper](https://arxiv.org/abs/1608.03983.pdf) by Ilya Loshchilov and Frank Hutter.\n",
    "\n",
    "* Snapshot ensembling, an extension to the above technique which involves taking a snapshot of the model after every cycle and using the last *M* snapshots as an ensemble. This technique is based on [this paper](https://arxiv.org/abs/1704.00109) by Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E. Hopcroft and Kilian Q. Weinberger.\n",
    "\n",
    "We will make use of the [PyTorch](http://pytorch.org) deep learning library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the necessary libraries\n",
    "First, we must import some external packages, including numpy, matplotlib and various PyTorch modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the hardware backend (GPU/CPU)\n",
    "Next, we set up the `device` global variable to allow automatic use of the GPU when available, with the option to force the use of the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_cpu = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and not force_cpu else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions and classes\n",
    "We will define some helper functions and classes to assist with keeping track of loss, accuracy and learning rate throughout training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mv_avg(l, n):\n",
    "    n = min(n, len(l))\n",
    "    s = sum(l[-n:])\n",
    "    return s / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes for keeping track of performance information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceHistory:\n",
    "    \n",
    "    def __init__(self, mv_avg_w=1):\n",
    "        self.losses = []\n",
    "        self.accs = []\n",
    "        self.mv_avg_w = mv_avg_w\n",
    "        self.mv_avg_losses = []\n",
    "        self.mv_avg_accs = []\n",
    "        \n",
    "    def update(self, loss, acc):\n",
    "        self.losses.append(loss)\n",
    "        mv_avg_loss = mv_avg(self.losses, self.mv_avg_w)\n",
    "        self.mv_avg_losses.append(mv_avg_loss)\n",
    "        self.accs.append(acc)\n",
    "        mv_avg_acc = mv_avg(self.accs, self.mv_avg_w)\n",
    "        self.mv_avg_accs.append(mv_avg_acc)\n",
    "        \n",
    "class TrainingHistory(PerformanceHistory):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(32)\n",
    "        self.lrs = []\n",
    "        \n",
    "    def update(self, lr, loss, acc):\n",
    "        super().update(loss, acc)\n",
    "        self.lrs.append(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset\n",
    "Next, we will set the batch size, set up data augmentation and create dataloaders for the CIFAR10 training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "training_set_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(32, scale=(0.9, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "training_set = datasets.CIFAR10(root='CIFAR10_data', train=True,\n",
    "                                transform=training_set_transform,\n",
    "                                download=True)\n",
    "training_set_loader = torch.utils.data.DataLoader(training_set,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=True,\n",
    "                                                  num_workers=4)\n",
    "\n",
    "test_set_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "test_set = datasets.CIFAR10(root='CIFAR10_data', train=False,\n",
    "                            transform=test_set_transform,\n",
    "                            download=True)\n",
    "test_set_loader = torch.utils.data.DataLoader(test_set,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the data\n",
    "To visualize the data, including augmentation, we can display a batch of images from the training set dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, _ = next(iter(training_set_loader))\n",
    "grid = torchvision.utils.make_grid(images, normalize=True)\n",
    "grid = np.transpose(grid.numpy(), (1, 2, 0))\n",
    "plt.imshow(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the network architecture\n",
    "We first define a class which inherits from `nn.Module` and represents a convolutional layer with batch normaliztion and a ReLU activation function, and another class representing the residual version of this. These are then used to create a simple resnet-style convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBnLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels,\n",
    "                 kernel_size, stride, padding):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size,\n",
    "                              stride, padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.relu(self.bn(self.conv(x)))\n",
    "\n",
    "class ResLayer(ConvBnLayer):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + super().forward(x)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvBnLayer(3, 32, 5, 1, 2)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            ConvBnLayer(32, 64, 3, 2, 1),\n",
    "            ResLayer(64, 64, 3, 1, 1),\n",
    "            ResLayer(64, 64, 3, 1, 1))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            ConvBnLayer(64, 128, 3, 2, 1),\n",
    "            ResLayer(128, 128, 3, 1, 1),\n",
    "            ResLayer(128, 128, 3, 1, 1)\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(4)\n",
    "        self.fc = nn.Linear(2048, 10)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.dropout(x.view(x.size(0), -1))\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the learning rate schedulers\n",
    "We will create two learning rate scheduler classes which inherit from a `_Scheduler` base class.  The `Exponential` scheduler simply multiplies the learning rate by a certain quantity each iteration, while the `CosineAnnealing` scheduler varies the learning rate cyclically in a cosine pattern. Like PyTorch's built-in learning rate schedulers, they contain an optimizer as an attribute and have a `step()` method which sets the learning rate of this optimizer, but here this is done on a per-minibatch basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Scheduler:\n",
    "    \n",
    "    def __init__(self, optimizer):\n",
    "        self.i = 0\n",
    "        self.optimizer = optimizer\n",
    "        self.lr = 0\n",
    "        self.snapshot = False\n",
    "        \n",
    "    def calc_lr(self):\n",
    "        return 0\n",
    "    \n",
    "    def step(self):\n",
    "        self.lr = self.calc_lr()\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = self.lr\n",
    "        self.i += 1\n",
    "        return self.snapshot\n",
    "\n",
    "class CosineAnnealing(_Scheduler):\n",
    "    \n",
    "    def __init__(self, optimizer, min_lr, max_lr, cycle_len, cycle_mult):\n",
    "        super().__init__(optimizer)\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.cycle_len = cycle_len\n",
    "        self.i_max = self.cycle_len - 1\n",
    "        self.cycle_mult = cycle_mult\n",
    "        \n",
    "    def calc_lr(self):\n",
    "        # linearly scale iteration to be between 0 and pi\n",
    "        # so cosine is between -1 and 1\n",
    "        x = self.i / self.i_max * np.pi\n",
    "        # take cosine of scaled iteration and linearly\n",
    "        # scale it to be between min_lr and max_lr\n",
    "        lr = (self.max_lr - self.min_lr) / 2 * (np.cos(x) + 1) + self.min_lr\n",
    "        return lr\n",
    "    \n",
    "    def step(self):\n",
    "        _ = super().step()\n",
    "        if self.i > self.i_max:\n",
    "            self.i = 0\n",
    "            self.cycle_len *= self.cycle_mult\n",
    "            self.i_max = self.cycle_len - 1\n",
    "            self.snapshot = True\n",
    "        else:\n",
    "            self.snapshot = False\n",
    "        return self.snapshot\n",
    "\n",
    "class Exponential(_Scheduler):\n",
    "    \n",
    "    def __init__(self, optimizer, base_lr=5e-6, n=1.01):\n",
    "        super().__init__(optimizer)\n",
    "        self.base_lr = base_lr\n",
    "        self.n = n\n",
    "        \n",
    "    def calc_lr(self):\n",
    "        lr = self.base_lr * self.n ** self.i\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the classification model\n",
    "Here, we implement the main image classifier class. It contains a model, an optimizer and objects for storing performance information. The `lr_find` method is used in assessing how learning rate affects training performance: it runs a short training cycle with an `Exponential` scheduler which quickly increases the learning rate, storing performance information in `lrf_history`. The `set_scheduler` method can then be used to create a scheduler based on this information. The `train` method trains the network for a given number of epochs, with the option to use snapshot ensembling during test set evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifier:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = CNN().to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters())\n",
    "        self.scheduler = None\n",
    "        self.snapshots = []\n",
    "        self.lrf_history = TrainingHistory()\n",
    "        self.train_history = TrainingHistory()\n",
    "        self.test_history = PerformanceHistory()\n",
    "\n",
    "    def forward_pass(self, model, data):\n",
    "        inputs, targets = data\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predictions = outputs.max(1)\n",
    "        loss = F.cross_entropy(outputs, targets)\n",
    "        acc = (predictions == targets).sum().item() / batch_size\n",
    "        return loss, acc\n",
    "    \n",
    "    def lr_find(self, epochs=1):\n",
    "        lrf_model = copy.deepcopy(self.model)\n",
    "        lrf_optimizer = optim.Adam(lrf_model.parameters())\n",
    "        lrf_scheduler = Exponential(lrf_optimizer)\n",
    "        for epoch in range(1, epochs+1):\n",
    "            with tqdm(training_set_loader,\n",
    "                      desc=\"[lr_find] Epoch %d/%d\" % (epoch, epochs),\n",
    "                      unit=\"batches\") as t:\n",
    "                for data in t:\n",
    "                    lrf_optimizer.zero_grad()\n",
    "                    loss, acc = self.forward_pass(lrf_model, data)\n",
    "                    loss.backward()\n",
    "                    _ = lrf_scheduler.step()\n",
    "                    lrf_optimizer.step()\n",
    "                    self.lrf_history.update(lrf_scheduler.lr,\n",
    "                                            loss.item(), acc)\n",
    "                    t.set_postfix(loss=self.lrf_history.mv_avg_losses[-1],\n",
    "                                  acc=self.lrf_history.mv_avg_accs[-1],\n",
    "                                  lr=lrf_scheduler.lr)\n",
    "                    starting_loss_i = min(31, len(self.lrf_history.mv_avg_losses)-1)\n",
    "                    loss_threshold = self.lrf_history.mv_avg_losses[starting_loss_i] * 1.3\n",
    "                    if self.lrf_history.mv_avg_losses[-1] > loss_threshold:\n",
    "                        break\n",
    "        print(\"Done!\")\n",
    "                        \n",
    "    def set_scheduler(self, scheduler, opts):\n",
    "        self.scheduler = scheduler(self.optimizer, *opts)\n",
    "\n",
    "    def train(self, epochs=10, snapshot_ensemble_size=0):\n",
    "        if self.scheduler == None:\n",
    "            print(\"Scheduler not yet set\")\n",
    "            return\n",
    "        for epoch in range(1, epochs+1):\n",
    "            with tqdm(training_set_loader,\n",
    "                      desc=\"[train] Epoch %d/%d\" % (epoch, epochs),\n",
    "                      unit=\"batches\") as t:\n",
    "                for data in t:\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss, acc = self.forward_pass(self.model, data)\n",
    "                    loss.backward()\n",
    "                    snapshot = self.scheduler.step()\n",
    "                    if snapshot_ensemble_size > 0 and snapshot:\n",
    "                        self.snapshots.append(copy.deepcopy(self.model))\n",
    "                        if len(self.snapshots) > snapshot_ensemble_size:\n",
    "                            del self.snapshots[0]\n",
    "                    self.optimizer.step()\n",
    "                    self.train_history.update(self.scheduler.lr,\n",
    "                                              loss.item(), acc)\n",
    "                    t.set_postfix(loss=self.train_history.mv_avg_losses[-1],\n",
    "                                  acc=self.train_history.mv_avg_accs[-1])\n",
    "            with torch.no_grad():\n",
    "                running_test_loss = 0.0\n",
    "                running_test_acc = 0.0\n",
    "                i = 1\n",
    "                if len(self.snapshots) == 0:\n",
    "                    test_ensemble = [self.model]\n",
    "                else:\n",
    "                    test_ensemble = self.snapshots\n",
    "                with tqdm(test_set_loader,\n",
    "                          desc=\"[test] Epoch %d/%d\" % (epoch, epochs),\n",
    "                          unit=\"batches\") as t:\n",
    "                    for data in t:\n",
    "                        inputs, targets = data\n",
    "                        inputs = inputs.to(device)\n",
    "                        targets = targets.to(device)\n",
    "                        mean_outputs = 0\n",
    "                        for model in test_ensemble:\n",
    "                            outputs = model(inputs)\n",
    "                            mean_outputs += outputs\n",
    "                        mean_outputs = mean_outputs / len(test_ensemble)\n",
    "                        _, predictions = mean_outputs.max(1)\n",
    "                        loss = F.cross_entropy(mean_outputs, targets)\n",
    "                        acc = (predictions == targets).sum().item() / batch_size\n",
    "                        running_test_loss += loss.item()\n",
    "                        running_test_acc += acc\n",
    "                        t.set_postfix(loss=running_test_loss/i,\n",
    "                                      acc=running_test_acc/i)\n",
    "                        i += 1\n",
    "                self.test_history.update(running_test_loss/len(test_set_loader),\n",
    "                                         running_test_acc/len(test_set_loader))\n",
    "        print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding optimal learning rate values\n",
    "After creating a classifier, we run `lr_find()` and plot the results to estimate good learning rate settings. Optimal values most likely lie in the range where the loss is decreasing (or the accuracy is increasing) rapidly and consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = ImageClassifier()\n",
    "classifier.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(classifier.lrf_history.lrs,\n",
    "             classifier.lrf_history.mv_avg_losses,\n",
    "             color=\"Red\")\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(classifier.lrf_history.lrs,\n",
    "             classifier.lrf_history.mv_avg_accs,\n",
    "             color=\"Green\")\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with a static learning rate\n",
    "To create a baseline, we will train a classifier using an `Exponetial` scheduler where `n = 1`, meaning the learning rate stays constant. We select a learning rate of 0.0002, a value towards the high end of the optimal range according to the above plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.set_scheduler(Exponential, (2e-4, 1))\n",
    "classifier.train(epochs=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing training performance\n",
    "We can visualize the changes in learning rate, loss and accuracy throughout training using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3)\n",
    "axes[0].plot(classifier.train_history.lrs, color=\"Blue\")\n",
    "axes[0].set_ylabel(\"Learning Rate\", color=\"Blue\")\n",
    "axes[1].plot(classifier.train_history.mv_avg_losses, color=\"Red\")\n",
    "axes[1].set_ylabel(\"Loss\", color=\"Red\")\n",
    "axes[2].plot(classifier.train_history.mv_avg_accs, color=\"Green\")\n",
    "axes[2].set_ylabel(\"Accuracy\", color=\"Green\")\n",
    "axes[2].set_xlabel(\"Mini-batch\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with a cosine annealing learning rate scheduler and snapshot ensembling\n",
    "Next, we create a new classifier and set up a cosine annealing learning rate scheduler with a cycle length equivalent to 2 epochs. We can refer to the above plots based on the results of `lr_find()` to decide on boundary learning rate values. Additionally, we use an ensemble of 4 snapshots for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier2 = ImageClassifier()\n",
    "classifier2.set_scheduler(CosineAnnealing, (1e-5, 4e-4, len(training_set_loader)*2, 1))\n",
    "classifier2.train(epochs=24, snapshot_ensemble_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing training performance\n",
    "Again, we can use matplotlib to visualize the changes in learning rate, loss and accuracy throughout training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3)\n",
    "axes[0].plot(classifier2.train_history.lrs, color=\"Blue\")\n",
    "axes[0].set_ylabel(\"Learning Rate\", color=\"Blue\")\n",
    "axes[1].plot(classifier2.train_history.mv_avg_losses, color=\"Red\")\n",
    "axes[1].set_ylabel(\"Loss\", color=\"Red\")\n",
    "axes[2].plot(classifier2.train_history.mv_avg_accs, color=\"Green\")\n",
    "axes[2].set_ylabel(\"Accuracy\", color=\"Green\")\n",
    "axes[2].set_xlabel(\"Mini-batch\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the results\n",
    "With both models trained, we can visually compare their test set performance using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2)\n",
    "axes[0].plot(np.arange(1, len(classifier.test_history.accs)+1),\n",
    "             classifier.test_history.losses, \n",
    "             color=\"Crimson\", label=\"Static LR\")\n",
    "axes[0].plot(np.arange(1, len(classifier2.test_history.accs)+1),\n",
    "             classifier2.test_history.losses, \n",
    "             color=\"Goldenrod\", label=\"CA w/ SE\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].legend()\n",
    "axes[1].plot(np.arange(1, len(classifier2.test_history.accs)+1),\n",
    "             classifier.test_history.accs,\n",
    "             color=\"Darkblue\", label=\"Static LR\")\n",
    "axes[1].plot(np.arange(1, len(classifier2.test_history.accs)+1),\n",
    "             classifier2.test_history.accs,\n",
    "             color=\"Darkgreen\", label=\"CA w/ SE\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Accuracy\")\n",
    "axes[1].legend()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
