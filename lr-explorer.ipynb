{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_cpu = False\n",
    "if torch.cuda.is_available() and not force_cpu:\n",
    "    use_gpu = True\n",
    "    FloatTensor = torch.cuda.FloatTensor\n",
    "    LongTensor = torch.cuda.LongTensor\n",
    "else:\n",
    "    use_gpu = False\n",
    "    FloatTensor = torch.FloatTensor\n",
    "    LongTensor = torch.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "training_set_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(32, scale=(0.9, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "training_set = datasets.CIFAR10(root='CIFAR10_data', train=True,\n",
    "                               transform=training_set_transform,\n",
    "                               download=True)\n",
    "training_set_loader = torch.utils.data.DataLoader(training_set,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=True,\n",
    "                                                 num_workers=4)\n",
    "\n",
    "test_set_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "test_set = datasets.CIFAR10(root='CIFAR10_data', train=False,\n",
    "                           transform=test_set_transform,\n",
    "                           download=True)\n",
    "test_set_loader = torch.utils.data.DataLoader(test_set,\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=False,\n",
    "                                             num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, _ = next(iter(training_set_loader))\n",
    "grid = torchvision.utils.make_grid(images, normalize=True)\n",
    "grid = np.transpose(grid.numpy(), (1, 2, 0))\n",
    "plt.imshow(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mv_avg(l, n):\n",
    "    n = min(n, len(l))\n",
    "    s = sum(l[-n:])\n",
    "    return s / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Scheduler:\n",
    "    \n",
    "    def __init__(self, optimizer):\n",
    "        self.i = 0\n",
    "        self.optimizer = optimizer\n",
    "        self.lrs = []\n",
    "        self.losses = []\n",
    "        self.mv_avg_losses = []\n",
    "        self.accs = []\n",
    "        self.mv_avg_accs = []\n",
    "        \n",
    "    def get_lr(self):\n",
    "        return 0\n",
    "    \n",
    "    def step(self):\n",
    "        lr = self.get_lr()\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        self.lrs.append(lr)\n",
    "        self.i += 1\n",
    "\n",
    "class CosineAnnealing(_Scheduler):\n",
    "    \n",
    "    def __init__(self, optimizer, min_lr, max_lr, cycle_len, cycle_mult):\n",
    "        super().__init__(optimizer)\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.cycle_len = cycle_len\n",
    "        self.i_max = self.cycle_len - 1\n",
    "        self.cycle_mult = cycle_mult\n",
    "        \n",
    "    def get_lr(self):\n",
    "        # linearly scale iteration to be between 0 and pi\n",
    "        # so cosine is between -1 and 1\n",
    "        x = self.i / self.i_max * np.pi\n",
    "        # take cosine of scaled iteration and linearly\n",
    "        # scale it to be between min_lr and max_lr\n",
    "        lr = (self.max_lr - self.min_lr) / 2 * (np.cos(x) + 1) + self.min_lr\n",
    "        return lr\n",
    "    \n",
    "    def step(self):\n",
    "        super().step()\n",
    "        if self.i > self.i_max:\n",
    "            self.i = 0\n",
    "            self.cycle_len *= self.cycle_mult\n",
    "            self.i_max = self.cycle_len - 1\n",
    "\n",
    "class Exponential(_Scheduler):\n",
    "    \n",
    "    def __init__(self, optimizer, base_lr=5e-6, n=1.01):\n",
    "        super().__init__(optimizer)\n",
    "        self.base_lr = base_lr\n",
    "        self.n = n\n",
    "        \n",
    "    def get_lr(self):\n",
    "        lr = self.base_lr * self.n ** self.i\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceHistory:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "        self.mv_avg_losses = []\n",
    "        self.accs = []\n",
    "        self.mv_avg_accs = []\n",
    "        \n",
    "    def update_history(self, loss, acc):\n",
    "        self.losses.append(loss)\n",
    "        mv_avg_loss = mv_avg(self.losses, 32)\n",
    "        self.mv_avg_losses.append(mv_avg_loss)\n",
    "        self.accs.append(acc)\n",
    "        mv_avg_acc = mv_avg(self.accs, 32)\n",
    "        self.mv_avg_accs.append(mv_avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBnLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels,\n",
    "                 kernel_size, stride, padding):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size,\n",
    "                              stride, padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.relu(self.bn(self.conv(x)))\n",
    "\n",
    "class ResLayer(ConvBnLayer):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + super().forward(x)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvBnLayer(3, 32, 5, 1, 2)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            ConvBnLayer(32, 64, 3, 2, 1),\n",
    "            ResLayer(64, 64, 3, 1, 1),\n",
    "            ResLayer(64, 64, 3, 1, 1))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            ConvBnLayer(64, 128, 3, 2, 1),\n",
    "            ResLayer(128, 128, 3, 1, 1),\n",
    "            ResLayer(128, 128, 3, 1, 1)\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(4)\n",
    "        self.fc = nn.Linear(2048, 10)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.dropout(x.view(x.size(0), -1))\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0002)\n",
    "scheduler = CosineAnnealing(optimizer, 1e-5, 5e-4, \n",
    "                            len(training_set_loader)*2, 1)\n",
    "train_history = PerformanceHistory()\n",
    "if use_gpu:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(data, model):\n",
    "    inputs, targets = data\n",
    "    inputs = Variable(inputs.type(FloatTensor))\n",
    "    targets = Variable(targets.type(LongTensor))\n",
    "    outputs = model(inputs)\n",
    "    _, predictions = outputs.max(1)\n",
    "    loss = F.cross_entropy(outputs, targets)\n",
    "    acc = (predictions == targets).sum().data[0] / batch_size\n",
    "    return loss, acc\n",
    "\n",
    "def train(epoch, epochs):\n",
    "    with tqdm(training_set_loader,\n",
    "              desc=\"[train] Epoch %d/%d\" % (epoch, epochs)) as t:\n",
    "        for data in t:\n",
    "            optimizer.zero_grad()\n",
    "            loss, acc = forward(data, model)\n",
    "            loss.backward()\n",
    "            scheduler.step()\n",
    "            train_history.update_history(loss.data[0], acc)\n",
    "            optimizer.step()\n",
    "            t.set_postfix(loss=train_history.mv_avg_losses[-1],\n",
    "                          acc=train_history.mv_avg_accs[-1])\n",
    "            \n",
    "def test(epoch, epochs):\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    i = 1\n",
    "    with tqdm(test_set_loader,\n",
    "              desc=\"[test] Epoch %d/%d\" % (epoch, epochs)) as t:\n",
    "        for data in t:\n",
    "            loss, acc = forward(data, model)\n",
    "            running_loss += loss.data[0]\n",
    "            running_acc += acc\n",
    "            t.set_postfix(loss=running_loss/i, acc=running_acc/i)\n",
    "            i += 1\n",
    "        \n",
    "def lr_find(epoch, epochs):\n",
    "    lrf_model = copy.deepcopy(model)\n",
    "    lrf_optimizer = optim.Adam(lrf_model.parameters())\n",
    "    lrf_history = PerformanceHistory()\n",
    "    lrf_scheduler = Exponential(lrf_optimizer)\n",
    "    with tqdm(training_set_loader) as t:\n",
    "        for data in t:\n",
    "            lrf_optimizer.zero_grad()\n",
    "            loss, acc = forward(data, lrf_model)\n",
    "            loss.backward()\n",
    "            lrf_scheduler.step()\n",
    "            lrf_optimizer.step()\n",
    "            lrf_history.update_history(loss.data[0], acc)\n",
    "            t.set_postfix(loss=lrf_history.mv_avg_losses[-1],\n",
    "                          acc=lrf_history.mv_avg_accs[-1],\n",
    "                          lr=lrf_scheduler.lrs[-1])\n",
    "            starting_loss_i = min(31, len(lrf_history.mv_avg_losses)-1)\n",
    "            loss_threshold = lrf_history.mv_avg_losses[starting_loss_i] * 1.3\n",
    "            if lrf_history.mv_avg_losses[-1] > loss_threshold:\n",
    "                break\n",
    "    return lrf_scheduler, lrf_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, h = lr_find(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, h = l\n",
    "plt.semilogx(s.lrs, h.mv_avg_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "for epoch in range(1, epochs+1):\n",
    "    train(epoch, epochs)\n",
    "    test(epoch, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3)\n",
    "axes[0].plot(scheduler.lrs, color=\"Blue\")\n",
    "axes[0].set_ylabel(\"Learning Rate\", color=\"Blue\")\n",
    "axes[1].plot(train_history.mv_avg_losses, color=\"Red\")\n",
    "axes[1].set_ylabel(\"Loss\", color=\"Red\")\n",
    "axes[2].plot(train_history.mv_avg_accs, color=\"Green\")\n",
    "axes[2].set_ylabel(\"Accuracy\", color=\"Green\")\n",
    "axes[2].set_xlabel(\"Mini-batch\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
